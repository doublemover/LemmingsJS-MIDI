#!/usr/bin/env node
/**
 * tools/search.js
 * ----------------------------------------
 * Local semantic search over embeddings.json
 *
 *  USAGE
 *    node tools/search.js "<query>"              # top-10, pretty printed
 *    node tools/search.js "<query>" -n 25        # top-25
 *    node tools/search.js "<query>" --json       # machine-readable JSON
 *
 *  DEPENDS ON
 *    - embeddings.json (generated by npm run index / scripts/build_index.js)
 *
 *  NOTE
 *    The build script normalises each chunk vector to unit length, so
 *    similarity = dot(queryVec, chunkVec)  →  [0,1]
 */

import fs from 'node:fs/promises';
import path from 'node:path';

// ------- tiny CLI arg parser (no deps) --------
const argv = process.argv.slice(2);
if (argv.length === 0) {
  console.error('Usage: node tools/search.js "<query>" [-n 15] [--json]');
  process.exit(1);
}
const query      = argv[0];
const limitIdx   = argv.indexOf('-n');
const limit      = (limitIdx !== -1 ? Number(argv[limitIdx + 1]) : 10) || 10;
const rawJson    = argv.includes('--json');

// ------- read embeddings --------
const EMB_PATH = path.resolve('embeddings.json');
const raw = JSON.parse(await fs.readFile(EMB_PATH, 'utf8'));

const { numDocs, vocab, chunks } = raw;
const vocabSize = Object.keys(vocab).length;

// Convert vocab string->idx to Map for fast lookup
const vocabMap = new Map(Object.entries(vocab));

// ------- helper: same tokenizer as build_index.js --------
const tokenize = text =>
  text.toLowerCase().split(/[^a-z0-9_]+/u).filter(Boolean);

// ------- build query vector (binary weight + unit-norm) --------
const qFreq = new Map();
for (const tok of tokenize(query)) {
  if (!vocabMap.has(tok)) continue;          // OOV word, skip
  qFreq.set(tok, 1 + (qFreq.get(tok) || 0)); // count
}
if (qFreq.size === 0) {
  console.error('⚠️  None of the query terms exist in the vocabulary.');
  process.exit(0);
}

const qVec = new Float32Array(vocabSize);
let qNorm = 0;
for (const [tok, tf] of qFreq) {
  const idx = vocabMap.get(tok);
  qVec[idx] = tf;
  qNorm += tf * tf;
}
qNorm = Math.sqrt(qNorm);
for (let i = 0; i < qVec.length; i++) qVec[i] /= qNorm;

// ------- compute similarities --------
/*  chunks[i].vector is a normalised Float32Array serialised to plain array
 *  We turn it back into Float32Array on the fly.  */
const scores = chunks.map(ch => {
  const vec = ch.vector;      // array of numbers (unit length)
  let dot = 0;
  for (const [tok, tf] of qFreq) {
    const idx = vocabMap.get(tok);
    dot += qVec[idx] * vec[idx]; // vec[idx] exists for all indices
  }
  return { score: dot, chunk: ch };
});

scores.sort((a, b) => b.score - a.score);
const top = scores.slice(0, limit).filter(r => r.score > 0);

// ------- enrich with preview snippet --------
const results = [];
for (const { score, chunk } of top) {
  const absPath = path.resolve(chunk.file);
  const fileTxt = await fs.readFile(absPath, 'utf8');
  const words   = tokenize(fileTxt);
  const slice   = words.slice(chunk.start, chunk.end).join(' ');
  const preview = slice.length > 140 ? slice.slice(0, 140) + '…' : slice;

  results.push({
    file: chunk.file,
    start: chunk.start,
    end: chunk.end,
    score: Number(score.toFixed(4)),
    preview
  });
}

// ------- output --------
if (rawJson) {
  console.log(JSON.stringify(results, null, 2));
} else {
  for (const r of results) {
    console.log(`\n\u001b[36m${r.file}\u001b[0m  [${r.start}-${r.end}]  ⚑ ${r.score}`);
    console.log(`  ${r.preview}`);
  }
  console.log(`\n${results.length} / ${chunks.length} chunks matched ≥ 0.`);
}
